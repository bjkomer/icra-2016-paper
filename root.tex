%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%2345678901234567890123456789012345678901234567890123456789012345678901234567890
%        1         2         3         4         5         6         7         8

\documentclass[letterpaper, 10 pt, conference]{ieeeconf}  % Comment this line out if you need a4paper

%\documentclass[a4paper, 10pt, conference]{ieeeconf}      % Use this line for a4 paper

\IEEEoverridecommandlockouts                              % This command is only needed if 
                                                          % you want to use the \thanks command

\overrideIEEEmargins                                      % Needed to meet printer requirements.

% See the \addtolength command later in the file to balance the column lengths
% on the last page of the document

% The following packages can be found on http:\\www.ctan.org
%\usepackage{graphics} % for pdf, bitmapped graphics files
\usepackage{epsfig} % for postscript graphics files
\usepackage{mathptmx} % assumes new font selection scheme installed
\usepackage{times} % assumes new font selection scheme installed
\usepackage{amsmath} % assumes amsmath package installed
%\usepackage{amssymb}  % assumes amsmath package installed

\usepackage{graphicx} % For including graphics N.B. pdftex graphics driver
\usepackage{subcaption} %TODO TEMP REMOVE
% Allow symbol for degrees
\usepackage{gensymb}
% Allow descriptions under tables
\usepackage[flushleft]{threeparttable}

\title{\LARGE \bf
Adaptive Quadcopter Control using Biologically Inspired Neural Networks
}


\author{Brent Komer$^{1}$% <-this % stops a space
\thanks{$^{1}$Brent Komer is with the Computational Neuroscience Research Group, Department of Systems Design Engineering,
        University of Waterloo, N2L 3G1, Canada
        {\tt\small bjkomer@uwaterloo.ca}}%
}


\begin{document}



\maketitle
\thispagestyle{empty}
\pagestyle{empty}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{abstract}

This paper explores the application of a biologically inspired learning method to produce an adaptive controller for a quadcopter.

\end{abstract}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{INTRODUCTION}

Humans have an exceptional ability to be able to adapt to their surroundings.
In particular the human motor control system is able to compensate for changes in forces, torques, and inertial effects on the body.
For example, when picking up an object such as a hammer, the weight of the hammer will apply external forces to the hand.
This will change the dynamic properties of the hand and arm movements, yet the human motor control system is able to easily compensate for these changes and accurately control movement with the object.
Even if an object has never been encountered before, the human brain is able to calculate the correct changes in timing and muscle tensions in order to skilfully manipulate the object.
The predictive capabilities of the brain, along with the plasticity of neural connections in the motor area help guide these sophisticated behaviours.


This ability for quick and easy adaptation to new dynamic properties of a system would be extremely useful in robotics. 
Applying similar methods of control that have been developed over millions of years of evolution in the brain to a robotic control system could result in major improvements. 
This is especially useful now that the demands of many robotic systems are now more general purpose than they were in the past. 
Robots started out mainly performing simple and repetitive tasks in stable environments, such as automation in manufacturing \cite{garcia2007evolution}. 
Now they are being used increasingly in more complex situations requiring a diverse amount of control, such as search and rescue missions, performing medical procedures, and assisting the elderly \cite{garcia2007evolution, hockstein2007history, nourbakhsh2005human, lacey1998application}. 
When the precise environment that the robot will operate in is not fully known, it is useful for any control system that the robot uses to be adaptable to those environments.

Another advantage the brain has when it comes to control, is that it uses very little power, about 20 Watts on average \cite{hart1975brain}.
% the motor is the majority of the power on a quadcopter, so neuromorphic hardware might not help much here
Hardware inspired by the brain is being designed to take advantage of this low power paradigm. 
This style of hardware, known as neuromorphic hardware, is typically massively parallel and consumes much less power than traditional hardware.
The algorithms explored in this thesis focus on being adaptive and biologically inspired in order to take advantage of such hardware.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{BACKGROUND}

\subsection{Quadcopter Dynamics}



\subsection{Adaptive Control}

Often the kinematics and dynamics of the system being controlled are unknown, or change over time in an unknown fashion. 
A controller that works well for the system initially may no longer be ideal when the system undergoes changes. 
In this situation, it is useful to have a controller that can adapt to these changes.

The foundation of adaptive control is based on parameter estimation.
First, a mathematical model of the system to be controlled is generated based on physical laws.
This model is typically of the form shown in \eqref{eq:physical_equation}, where $q$ is the vector of state variables, $M(q)$ is a mass/inertia matrix, $C(q,\dot{q})$ is the coriolis-drag term, $g(q)$ is the gravitational force, and $\tau$ is a vector representing the input force/torque to the system.

\begin{equation} \label{eq:physical_equation}
M(q)\ddot{q} + C(q,\dot{q})\dot{q} + g(q) = \tau
\end{equation}

%TODO
%[[[[[a lot more stuff to go here, need to understand it more first]]]]]
The goal of the controller is to bring the system to a particular target state.
Typically the state as well as its derivative is desired to be controlled.
Thus, the second derivative of the state will be zero when the system has arrived at the target state.
Setting $\ddot{q}$ to be zero gives the relationship of the inputs to the rest of the state shown in \eqref{eq:input_equation}.

\begin{equation} \label{eq:input_equation}
\tau = C(q,\dot{q})\dot{q} + g(q)
\end{equation}

An estimate of $q$ and $\dot{q}$ can typically be measured, leaving the only potentially unknown quantities in the right-hand side of the equation to be the physical parameters of the system.
If these physical parameters are constant and linear with respect to the system state, the equation can be reorganized as in \eqref{eq:Y_equation}.
Here $\theta$ is a vector of constant system parameters and $Y(q,\dot{q})$ is a known matrix dependent on the system state. 
$\tau$ is the input required to keep the system in a steady state.

\begin{equation} \label{eq:Y_equation}
\tau = C(q,\dot{q})\dot{q} + g(q) = Y(q,\dot{q})\theta
\end{equation}

Often the system parameters are not fully known and an estimate needs to be used instead.
Many control applications also require the system to be able to transition to different states, rather than remain at a particular state.
The adaptive control law in \eqref{eq:Y_and_control_law} uses an estimate of the parameter vector, $\hat{\theta}$, along with a standard control law to compute the desired input to the system. Here $e$ is the state error and $K$ is a gain matrix.
More detail on this style of adaptive control law can be found in \cite{slotine1987adaptive, slotine1991applied, cheah2006adaptive}.
%TODO EDIT1 add references to Slotine papers as well as a couple quick comments about the strengths of those controllers (guaranteed to converge with sufficient exploration of space, etc)

\begin{equation} \label{eq:Y_and_control_law}
\tau = Y(q,\dot{q})\hat{\theta} + Ke
\end{equation}

The parameter estimates can be initialized to any stable value and are updated according to the relationship in \eqref{eq:Y_update_equation}.
$L$ is a learning rate parameter that determines how quickly the parameter estimates change over time in proportion to the measured error.
The parameter estimates will eventually converge on values that allow the system to be controlled with minimal error.
Given sufficient exploration of the state space, the parameter estimates are guaranteed to converge to the real values if the real values are required for optimal control \cite{slotine1987adaptive}.

\begin{equation} \label{eq:Y_update_equation}
\dot{\hat{\theta}} = LY(q,\dot{q})^{T}Ke
\end{equation}


Creating a mathematical model of a system with enough detail to account for everything is difficult. 
Assumptions and approximations must be made if the model is to be tractable. 
Moreover, external forces from the environment may influence the model, and their form may be unknown as the environment can be largely unknown. 
One way to overcome this problem is to use a set of basis functions as the model, and the weights applied to each element of the basis as the constant parameters. 
If the basis is designed such that it can represent any computable function to a reasonable degree of accuracy, it will be effective in the adaptive control problem. 
Gaussian basis functions are commonly used in adaptive control \cite{sanner1992gaussian}, but neural networks may be used as well \cite{barto1983neuronlike}.
This thesis explores the application of this form of control law using basis functions that are biologically plausible spiking neurons.

\subsection{Neural Simulation}

The Neural Engineering Framework (NEF)\cite{eliasmith2004neural} provides a means of representing arbitrary vectors using the properties of neurons as a basis.
This is done through a nonlinear encoding mechanism carried out by the tuning curves of the neurons, and a weighted linear decoding of the responses of the neurons to retrieve an approximation of the vector being encoded.
A transformation can be applied to the underlying representation by specifying different weights on the linear decoding.
Any computable function can be approximated through a transform, and the degree of accuracy of the decoding is dependent on the number of neurons used and the complexity of the function.
%TODO put some simple example here about x and x**2?
%TODO talk about dynamics here, might need to fix wording, and talk about recurrent connections?
The neurons themselves are a part of a dynamical system where timing effects and filters across connections play a role in the behaviour of the system. 
%It also specifies a means of performing transformation
For more detail on the NEF, see \cite{eliasmith2007build, stewart2011neural, eliasmith2013build}. % put this line in if you think the reader might want more details.

Simulation of biological neurons is carried out by the software package Nengo \cite{bekolay2013nengo}.
This software implements the algorithms in the NEF and provides an easy to use Python interface for building complex models under this framework.
The core components of Nengo are networks, nodes, ensembles, and connections.
A network is a container for all of the components, it can contain any number of nodes, ensembles, connections, and even other networks.
There is always one base network from which the rest of the model is built.
Ensembles are groups of neurons representing a single vector. 
The dimensionality of this vector can be any positive integer.
Nodes are used when a particular part of the network is doing a computation without using neurons as the underlying representation.
Typically nodes are used as the inputs and outputs to a neural system.
Connections specify transformations between representational components (ensembles and nodes) through one-way links where the information flows from the output of the first representational component (origin) to the input of the second representational component (termination).
Connections may have a synapse model applied to them, where the information from one end of the connection is delayed by a time-step before reaching the other end and a filter with a particular time constant may be applied.
If no synaptic filter is applied, the value from the origin of the connection is sent directly to the termination of the connection during the same time step.

Nengo supports a variety of underlying neuron models for its Ensembles.
The most commonly used is the Leaky Integrate-and-Fire (LIF) neuron \cite{burkitt2006review}.
%[add a brief description?]. %TODO add this description
Ensembles can also be run in direct mode, in which functions are computed explicitly rather than with neurons.
However, models in this mode are not implementable directly on neuromorphic hardware, and the behaviour of the system can be significantly different.
Nevertheless, it can be useful for constructing working prototypes in simulation before converting the entire system to an underlying neural model. %TODO: this sounds gross, fix it... %TODO EDIT1

Once the network structure has been specified, Nengo can build and run a simulation of this network for either a specified duration of time, or until a stop signal is generated. 
All timing is measured as `simulated time' with a specific time-step. 
If the time-step is short, the simulation can capture minute timing details more accurately, but the system as a whole will run slower with respect to real-time. A default time step of 1ms is typically used in Nengo models, which provides a reasonable trade-off between accuracy and run-time.
%TODO is there any justification for this?

\subsection{Adaptive Control with Nengo}

In this thesis, we use the adaptive control methods described above to build a quadcopter controller using Nengo. 
An ensemble of simulated neurons is used as the set of basis functions for the physical model, and the decoders of these neurons are used as the vector of unknown constant parameters. 
A biologically plausible learning rule known as the Prescribed Error Sensitivity (PES) rule is used to update the decoder values \cite{bekolay2013simultaneous}. 

This learning method works by first creating a connection from an ensemble of spiking neurons representing the state of the system to an ensemble or node representing the output of the controller. 
This is known as the `learned connection' and can be initially set to perform any transformation, but is typically initialized for the output to be random or zero. 
If the designer has an approximation of what the final learned transformation should look like, they can set this as the initial transformation. 
Doing so will allow the system to converge to the final transformation more quickly.

The learned connection will be modulated by an error signal, which can come from anywhere in the network. 
The PES learning rule will attempt to reduce the error signal by changing the value of the decoders on the learned connection. 
The direction in which the decoder values change is dependent on the sign of the error. 
The magnitude of the change in decoder values at each time step is dependent on both the magnitude of the error and a learning-rate parameter. 
The learning rate is a dimensionless parameter that needs to be tuned for the specific application. 
It is dependent on the simulation time step, the number of neurons in the state ensemble, as well as how responsive the model needs to be to changes. 
%TODO [[check to make sure this is true!!]] 
A larger learning rate will cause larger reactions to error, effectively making the system trust its current measurements more than historical ones. 
A smaller time-step means that these changes will occur more frequently, so the net change over time will be greater. 
A larger number of neurons means that the changes will be greater, as there will be more decoders changing. The overall transformation is a sum of these decoders. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONTROLLER DESIGN}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EXPERIMENTS}

% Put in loop figures in this section somewhere

\subsection{Metrics}

Three metrics are used to evaluate performance on these tasks. 
The first is the Root Mean Squared (RMS) error of the difference between the quadcopter's current state and its target state.
The quadcopter's state consists of position, velocity, orientation, and angular velocity.
These state variables are combined by computing the length of the resulting 12-dimensional error vector to produce a single quantity.
This value is calculated at each time-step for the duration of the run and then averaged by the number of time-steps in the run.
For point to point control this error is sometimes not the most informative because as soon as the target point has changed a large error value will be recorded even if the quadcopter is moving optimally towards its target.

The second metric is a modified version of the RMS error designed to take the desired trajectory into account.
This works by ignoring any error along the direction to the target as long as the current velocity is also in that direction.
It also ignores any error caused by the velocity in the correct direction as long as the quadcopter is not currently at its target.
There are also weights placed on specific types of errors to reflect an increased desire to minimize those errors.
For example, errors caused by overshooting the target are weighted more heavily.

The third metric is the time taken for the quadcopter to reach its target within a particular tolerance. 
This metric favours controllers that can reach the target quickly.
This metric does not worry about overshoot and non-optimal trajectories as long as steady state is achieved at the target in the end.
The particular tolerance chosen for these experiments is maintaining an RMS error of less than 0.001 for a duration of one second.

The majority of the benchmarks performed in this thesis will report results using the trajectory RMS error and the time-to-target metric.
The RMS metrics were recorded over a 30 second simulation time.
The time-to-target trials were also run for a total of 30 seconds and a value of 30 is recorded if the quadcopter never reaches its target within tolerance over that duration.

\subsection{Benchmarks}
\subsection{Improvement over Time}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{CONCLUSIONS}


\addtolength{\textheight}{-12cm}   % This command serves to balance the column lengths
                                  % on the last page of the document manually. It shortens
                                  % the textheight of the last page by a suitable amount.
                                  % This command does not take effect until the next page
                                  % so it should come on the page before the last. Make
                                  % sure that you do not shorten the textheight too much.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{APPENDIX}

Appendixes should appear before the acknowledgment.

\section*{ACKNOWLEDGMENT}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,root}


\end{document}
